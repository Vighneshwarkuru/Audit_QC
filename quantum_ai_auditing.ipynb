{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf510b26",
   "metadata": {},
   "source": [
    "# Quantum-Enhanced AI Auditing Notebook\n",
    "\n",
    "This notebook demonstrates how to **audit multiple AI models** trained on a recruitment dataset for **bias and ethical violations** using **quantum-inspired techniques**.\n",
    "\n",
    "### Features\n",
    "- Train multiple classical AI models (Logistic Regression, Random Forest, Neural Net)\n",
    "- Audit them with quantum-enhanced methods:\n",
    "  - Bias detection via quantum kernels\n",
    "  - Encrypted auditing (homomorphic-style simulation)\n",
    "  - Adversarial robustness checks\n",
    "  - Provable fairness certificate generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3336c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install required libraries if missing\n",
    "# !pip install pennylane scikit-learn matplotlib seaborn numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as qnp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771e1ee5",
   "metadata": {},
   "source": [
    "## 1. Load and Inspect Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb977e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load recruitment dataset\n",
    "df = pd.read_csv(\"/mnt/data/recruitment_data.csv\")\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9ce9d4",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6b2f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For demonstration, assume 'hired' is the target variable (adjust if needed)\n",
    "target = 'hired'  # change if different\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "# Encode categorical variables if any\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"Preprocessing complete. Features shape:\", X_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a1e435",
   "metadata": {},
   "source": [
    "## 2. Train Multiple AI Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933a0705",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"Neural Net\": MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    results[name] = classification_report(y_test, y_pred, output_dict=True)\n",
    "    print(f\"\\nModel: {name}\\n\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5d72ee",
   "metadata": {},
   "source": [
    "## 3. Quantum-Enhanced Bias Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a42de5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: use quantum kernel to estimate similarity and check for subgroup bias\n",
    "from pennylane.kernels import square_kernel_matrix, kernel_matrix\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=2)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def kernel_circuit(x1, x2):\n",
    "    qml.templates.AngleEmbedding(x1, wires=[0,1])\n",
    "    qml.adjoint(qml.templates.AngleEmbedding)(x2, wires=[0,1])\n",
    "    return qml.probs(wires=[0])\n",
    "\n",
    "def quantum_kernel(X):\n",
    "    X = qnp.array(X, requires_grad=False)\n",
    "    return square_kernel_matrix(X, kernel_circuit)\n",
    "\n",
    "# Subsample small set due to quantum sim cost\n",
    "sample_idx = np.random.choice(len(X_test), size=20, replace=False)\n",
    "X_sample = X_test[sample_idx]\n",
    "y_sample = y_test.iloc[sample_idx]\n",
    "\n",
    "K = quantum_kernel(X_sample)\n",
    "print(\"Quantum kernel matrix shape:\", K.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a02194",
   "metadata": {},
   "source": [
    "## 4. Encrypted Auditing Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc5d92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simulate encrypted auditing by transforming parameters before auditing\n",
    "def encrypt_params(params, key=42):\n",
    "    return np.array(params) * key\n",
    "\n",
    "def decrypt_params(params, key=42):\n",
    "    return np.array(params) / key\n",
    "\n",
    "for name, model in models.items():\n",
    "    if hasattr(model, 'coef_'):\n",
    "        encrypted = encrypt_params(model.coef_)\n",
    "        decrypted = decrypt_params(encrypted)\n",
    "        print(f\"Model: {name} - Encryption/Decryption consistency check:\", np.allclose(model.coef_, decrypted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4605589c",
   "metadata": {},
   "source": [
    "## 5. Adversarial Robustness Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a44ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple FGSM-style perturbation simulation\n",
    "epsilon = 0.1\n",
    "X_adv = X_test + epsilon * np.sign(np.random.randn(*X_test.shape))\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred_clean = model.predict(X_test)\n",
    "    y_pred_adv = model.predict(X_adv)\n",
    "    diff = np.mean(y_pred_clean != y_pred_adv)\n",
    "    print(f\"Model: {name} - Adversarial attack flip rate: {diff:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5321dbff",
   "metadata": {},
   "source": [
    "## 6. Fairness Certificate Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c22aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Provable fairness certificate: demographic parity gap\n",
    "def demographic_parity(y_true, y_pred, sensitive_attr):\n",
    "    groups = df.loc[y_true.index, sensitive_attr]\n",
    "    vals = []\n",
    "    for g in groups.unique():\n",
    "        mask = (groups == g)\n",
    "        vals.append(np.mean(y_pred[mask]))\n",
    "    return max(vals) - min(vals)\n",
    "\n",
    "sensitive_attr = df.columns[0]  # Example: first column (change if needed)\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    dp_gap = demographic_parity(y_test, y_pred, sensitive_attr)\n",
    "    print(f\"Model: {name} - Demographic Parity Gap: {dp_gap:.3f}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
